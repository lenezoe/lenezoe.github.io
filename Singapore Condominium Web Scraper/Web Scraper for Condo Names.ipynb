{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f92fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import urllib\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76193ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set wait times\n",
    "waittime = 30\n",
    "sleeptime = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e205cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate web driver\n",
    "try:\n",
    "    driver.close()  # Close any existing WebDrivers\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set webdriver options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('ignore-certificate-errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to Chrome WebDriver executable depending on where your chromedriver is stored\n",
    "webdriver_path = r'C:\\WebDriver\\chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate webdriver with the specified path\n",
    "driver = webdriver.Chrome(executable_path=webdriver_path, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target website. We are using 99.co's listing of condominiums. Page was last checked on 6 Jun\n",
    "home_page = \"https://www.99.co/singapore/condos-apartments?alphabet=a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get driver to retrieve homepage\n",
    "driver.get(home_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep a short while for page loading to be fully completed\n",
    "time.sleep(sleeptime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse and display HTML of results page 1, to take a look\n",
    "results_html = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "results_html.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffbae1a8",
   "metadata": {},
   "source": [
    "(1) Test the code on the first page before scraping all pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pty_name = driver.find_elements(By.XPATH, \"//div[@class='_2IBku _32zfS']\")\n",
    "pty_name_list = []\n",
    "for i in pty_name:\n",
    "    pty_name_list.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6001d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the scraped data\n",
    "data = pd.DataFrame({'name': pty_name_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff261119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "file_name = 'condonames.csv'\n",
    "data.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f292b9b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"Scraped data saved to {file_name}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65cd7252",
   "metadata": {},
   "source": [
    "(2) Create fucnction to get current page number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d421225",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_current_page():\n",
    "    WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, \"//li[@class='isActive']/a\")))\n",
    "\n",
    "    current_page_element = driver.find_element(By.XPATH, \"//li[@class='isActive']/a\")\n",
    "    current_page = current_page_element.text\n",
    "\n",
    "    return current_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01315324",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "get_current_page()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5b277d8",
   "metadata": {},
   "source": [
    "(3)  Create fucnction to get absolute last page number which is last element before the next arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9d445",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_absolute_last_page():\n",
    "    WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, \"//li/a[@aria-label]\")))\n",
    "\n",
    "    page_elements = driver.find_elements(By.XPATH, \"//li/a[@aria-label]\")\n",
    "    last_page_element = page_elements[-1]\n",
    "    last_page_number = last_page_element.text\n",
    "\n",
    "    return last_page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d5854c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "get_absolute_last_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021023b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# (4) Create scraper function\n",
    "def scrape_pages():\n",
    "    current_page = int(get_current_page())\n",
    "    last_page = int(get_absolute_last_page())\n",
    "    alphabet = 'a'\n",
    "\n",
    "    pty_name_list = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            pty_name = driver.find_elements(By.XPATH, \"//div[@class='_2IBku _32zfS']\")\n",
    "\n",
    "            for i in pty_name:\n",
    "                pty_name_list.append(i.text)\n",
    "\n",
    "            if current_page == last_page:\n",
    "                if alphabet == 'x':\n",
    "                    alphabet = chr(ord(alphabet) + 1)  # Assuming 'x' has no results, skip 'x' and move to the next alphabet\n",
    "                elif alphabet == 'z':\n",
    "                    alphabet = '%23'  # Set alphabet to '%23' after 'z'\n",
    "                else:\n",
    "                    alphabet = chr(ord(alphabet) + 1)  # Increment the alphabet\n",
    "                next_page = 1  # Reset the page number\n",
    "            else:\n",
    "                next_page = current_page + 1  # Go to the next page\n",
    "\n",
    "            next_url = f\"https://www.99.co/singapore/condos-apartments?alphabet={alphabet}&page={next_page}\"\n",
    "            driver.get(next_url)\n",
    "\n",
    "            # Update the current page and last page for the new alphabet\n",
    "            current_page = int(get_current_page())\n",
    "            last_page = int(get_absolute_last_page())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break  # Exit the loop if an error occurs\n",
    "\n",
    "    # Create a DataFrame with the scraped data\n",
    "    data = pd.DataFrame({'Name': pty_name_list})\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    file_name = 'condonames.csv'\n",
    "    data.to_csv(file_name, index=False)\n",
    "\n",
    "    print(f\"Scraped data saved to {file_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the scrape_pages() function\n",
    "scrape_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea5a00",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Display time concluded\n",
    "print(time.strftime(\"%H:%M:%S\", time.localtime()))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
